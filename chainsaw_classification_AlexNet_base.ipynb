{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fWomgC-kF5f"
   },
   "source": [
    "**Architecture **\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=12JomC2IswVbNGdE0IIvPpUk8vPjP-MBQ\"  alt=\"artchtecture\">\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uN7hQRZsDbgI"
   },
   "source": [
    "(1) Importing dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3lPxjI5BDAkX",
    "outputId": "88280284-3c51-485b-adfa-4c428507fb92"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,\\\n",
    "                         Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "import random\n",
    "random.seed(13)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "py5KMVLnDZsC"
   },
   "source": [
    "(2) Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/data/private/SU/bbchip13/chainsaw_classification/data/'\n",
    "chainsaw_wav_dir = base_dir+'chainsaw/'\n",
    "other_wav_dir = base_dir+'no_chainsaw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainsaw_list = [chainsaw_wav_dir+filename for filename in os.listdir(chainsaw_wav_dir) \n",
    "                 if filename.endswith('.wav')]\n",
    "chainsaw_list = sklearn.utils.shuffle(chainsaw_list)\n",
    "# chainsaw_list = chainsaw_list[:1200]\n",
    "chainsaw_list = chainsaw_list\n",
    "chainsaw_labels = np.ones(len(chainsaw_list))\n",
    "\n",
    "no_chainsaw_list = [other_wav_dir+filename for filename in os.listdir(other_wav_dir)\n",
    "                    if filename.endswith('.wav')]\n",
    "no_chainsaw_list = sklearn.utils.shuffle(no_chainsaw_list)\n",
    "# no_chainsaw_list = no_chainsaw_list[:1200]\n",
    "no_chainsaw_list = no_chainsaw_list\n",
    "no_chainsaw_labels = np.zeros(len(no_chainsaw_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wavs(filenames):\n",
    "    return np.asarray([librosa.load(filename)[0] for filename in tqdm(filenames)])\n",
    "\n",
    "### If you have lack of memory, Use this\n",
    "#     wav = librosa.load(filenames[0])\n",
    "#     wavs = np.zeros( (len(filenames), wav.shape[0]) )\n",
    "#     for i, filename in enumerate(filenames):\n",
    "#         wavs[i][:] = librosa.load(filename)[:]\n",
    "#     return wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Train Data......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adba7efe92b44e0ada4abc381741220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2218), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Test Data......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1f7f8d71524d7d942d08740281c702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1093), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2218, 110250, 1) (2218, 1) (1093, 110250, 1) (1093, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train_chainsaw, x_test_chainsaw, y_train_chainsaw, y_test_chainsaw \\\n",
    "    = train_test_split(chainsaw_list, chainsaw_labels, test_size = 0.33, random_state = 7)\n",
    "x_train_no_chainsaw, x_test_no_chainsaw, y_train_no_chainsaw, y_test_no_chainsaw \\\n",
    "    = train_test_split(no_chainsaw_list, no_chainsaw_labels, test_size = 0.33, random_state = 7)\n",
    "\n",
    "x_train_filenames = x_train_chainsaw+x_train_no_chainsaw\n",
    "y_train = np.concatenate([y_train_chainsaw, y_train_no_chainsaw])\n",
    "\n",
    "x_test_filenames = x_test_chainsaw+x_test_no_chainsaw\n",
    "y_test = np.concatenate([y_test_chainsaw, y_test_no_chainsaw])\n",
    "\n",
    "x_train_filenames, y_train = sklearn.utils.shuffle(x_train_filenames, y_train)\n",
    "x_test_filenames, y_test = sklearn.utils.shuffle(x_test_filenames, y_test)\n",
    "\n",
    "print('Load Train Data......')\n",
    "x_train = load_wavs(x_train_filenames)\n",
    "print('Load Test Data......')\n",
    "x_test = load_wavs(x_test_filenames)\n",
    "\n",
    "x_train = np.reshape(x_train, (*x_train.shape, 1))\n",
    "y_train = np.reshape(y_train, (*y_train.shape, 1))\n",
    "x_test = np.reshape(x_test, (*x_test.shape, 1))\n",
    "y_test = np.reshape(y_test, (*y_test.shape, 1))\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12cS85jvDnfS"
   },
   "source": [
    "(3) Create a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4GDedqMJcJYr"
   },
   "outputs": [],
   "source": [
    "### Define Parametric Softplus\n",
    "\n",
    "# alpha * log(1 + exp(beta * x))\n",
    "def ParametricSoftplus(alpha=0.2, beta=5.0):\n",
    "  return lambda x: alpha * keras.activations.softplus(beta * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "colab_type": "code",
    "id": "fs8Heys2Dm30",
    "outputId": "bad14ede-be9c-4a2f-d052-9d9a29a5e437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 6891, 48)          5856      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 6891, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6891, 48)          192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1723, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 1723, 128)         153728    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1723, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1723, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 431, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 431, 192)          221376    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 431, 192)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 431, 192)          331968    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 431, 192)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 431, 128)          221312    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 431, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 108, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 13824)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              56627200  \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 4097      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 74,347,553\n",
      "Trainable params: 74,347,201\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "# 1st Convolutional Layer (conv1)\n",
    "model.add(Conv1D (kernel_size=121, filters=48, strides=16, padding='same',\n",
    "#                   input_shape=x_train.shape[1:]))\n",
    "                  input_shape=(110250, 1)))\n",
    "\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Pooling (pool1)\n",
    "model.add(MaxPooling1D(pool_size=9, strides=4, padding='same'))\n",
    "\n",
    "# 2nd Convolutional Layer (conv2)\n",
    "model.add(Conv1D (kernel_size=25, filters=128, padding='same'))\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Pooling (pool2)\n",
    "model.add(MaxPooling1D(pool_size=9, strides=4, padding='same'))\n",
    "\n",
    "# 3rd Convolutional Layer (conv3)\n",
    "model.add(Conv1D (kernel_size=9, filters=192, padding='same'))\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "\n",
    "# 4rd Convolutional Layer (conv4)\n",
    "model.add(Conv1D (kernel_size=9, filters=192, padding='same'))\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "\n",
    "# 5rd Convolutional Layer (conv5)\n",
    "model.add(Conv1D (kernel_size=9, filters=128, padding='same'))\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "\n",
    "# Pooling (pool5)\n",
    "model.add(MaxPooling1D(pool_size=9, strides=4, padding='same'))\n",
    "\n",
    "# 1st Dense Layer (full6)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "model.add(Dropout(0.5)) # Drop-out value is not specified in the paper\n",
    "\n",
    "# 2nd Dense Layer (full7)\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output Layer (full8)\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLxfqHNxDuJq"
   },
   "source": [
    "(4) Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jPB8IbZDxeJ"
   },
   "outputs": [],
   "source": [
    "adam_with_params = keras.optimizers.Adam(lr=0.0002, beta_1=0.1, beta_2=0.999, \n",
    "                                         epsilon=1e-8)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam_with_params,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUsuRj-7Dzxx"
   },
   "source": [
    "(5) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'AlexNet_based_check_point/'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "model_filename = model_path+'{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "ZUVV71K2D2tZ",
    "outputId": "7a454152-003e-4615-acd8-cfdb60ef8170",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1552 samples, validate on 666 samples\n",
      "Epoch 1/100\n",
      "1552/1552 [==============================] - 8s 5ms/step - loss: 0.6601 - acc: 0.6405 - val_loss: 0.7874 - val_acc: 0.6096\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78736, saving model to AlexNet_based_check_point/01-0.7874.hdf5\n",
      "Epoch 2/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.5713 - acc: 0.7152 - val_loss: 0.5474 - val_acc: 0.7237\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78736 to 0.54742, saving model to AlexNet_based_check_point/02-0.5474.hdf5\n",
      "Epoch 3/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.2711 - acc: 0.9008 - val_loss: 0.4567 - val_acc: 0.8363\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54742 to 0.45670, saving model to AlexNet_based_check_point/03-0.4567.hdf5\n",
      "Epoch 4/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.2147 - acc: 0.9182 - val_loss: 0.1649 - val_acc: 0.9414\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.45670 to 0.16485, saving model to AlexNet_based_check_point/04-0.1649.hdf5\n",
      "Epoch 5/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1856 - acc: 0.9369 - val_loss: 0.1634 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16485 to 0.16343, saving model to AlexNet_based_check_point/05-0.1634.hdf5\n",
      "Epoch 6/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1818 - acc: 0.9336 - val_loss: 0.1853 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.16343\n",
      "Epoch 7/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1593 - acc: 0.9472 - val_loss: 0.1408 - val_acc: 0.9414\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.16343 to 0.14084, saving model to AlexNet_based_check_point/07-0.1408.hdf5\n",
      "Epoch 8/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1536 - acc: 0.9459 - val_loss: 0.1348 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.14084 to 0.13480, saving model to AlexNet_based_check_point/08-0.1348.hdf5\n",
      "Epoch 9/100\n",
      "1552/1552 [==============================] - 5s 3ms/step - loss: 0.1485 - acc: 0.9485 - val_loss: 0.1186 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.13480 to 0.11857, saving model to AlexNet_based_check_point/09-0.1186.hdf5\n",
      "Epoch 10/100\n",
      "1552/1552 [==============================] - 5s 3ms/step - loss: 0.1442 - acc: 0.9485 - val_loss: 0.1499 - val_acc: 0.9354\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.11857\n",
      "Epoch 11/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1423 - acc: 0.9504 - val_loss: 0.1653 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11857\n",
      "Epoch 12/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1520 - acc: 0.9510 - val_loss: 0.1537 - val_acc: 0.9339\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11857\n",
      "Epoch 13/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1364 - acc: 0.9497 - val_loss: 0.3680 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.11857\n",
      "Epoch 14/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1527 - acc: 0.9510 - val_loss: 0.1406 - val_acc: 0.9520\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11857\n",
      "Epoch 15/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1190 - acc: 0.9594 - val_loss: 0.1102 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11857 to 0.11021, saving model to AlexNet_based_check_point/15-0.1102.hdf5\n",
      "Epoch 16/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1097 - acc: 0.9639 - val_loss: 0.1331 - val_acc: 0.9459\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11021\n",
      "Epoch 17/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1083 - acc: 0.9613 - val_loss: 0.3162 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11021\n",
      "Epoch 18/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1496 - acc: 0.9543 - val_loss: 0.1267 - val_acc: 0.9459\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11021\n",
      "Epoch 19/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1099 - acc: 0.9613 - val_loss: 0.1098 - val_acc: 0.9565\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.11021 to 0.10980, saving model to AlexNet_based_check_point/19-0.1098.hdf5\n",
      "Epoch 20/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1300 - acc: 0.9568 - val_loss: 0.1139 - val_acc: 0.9489\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10980\n",
      "Epoch 21/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0987 - acc: 0.9671 - val_loss: 0.1164 - val_acc: 0.9489\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10980\n",
      "Epoch 22/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1088 - acc: 0.9633 - val_loss: 0.0893 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.10980 to 0.08926, saving model to AlexNet_based_check_point/22-0.0893.hdf5\n",
      "Epoch 23/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0990 - acc: 0.9691 - val_loss: 0.1496 - val_acc: 0.9414\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.08926\n",
      "Epoch 24/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1058 - acc: 0.9659 - val_loss: 0.1238 - val_acc: 0.9429\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.08926\n",
      "Epoch 25/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0907 - acc: 0.9710 - val_loss: 0.1084 - val_acc: 0.9520\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.08926\n",
      "Epoch 26/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1002 - acc: 0.9671 - val_loss: 0.0999 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.08926\n",
      "Epoch 27/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1059 - acc: 0.9659 - val_loss: 0.1008 - val_acc: 0.9550\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.08926\n",
      "Epoch 28/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0959 - acc: 0.9659 - val_loss: 0.5986 - val_acc: 0.8078\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.08926\n",
      "Epoch 29/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0921 - acc: 0.9665 - val_loss: 0.1102 - val_acc: 0.9520\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.08926\n",
      "Epoch 30/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1053 - acc: 0.9659 - val_loss: 0.5875 - val_acc: 0.8048\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.08926\n",
      "Epoch 31/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0938 - acc: 0.9665 - val_loss: 0.1466 - val_acc: 0.9414\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.08926\n",
      "Epoch 32/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1099 - acc: 0.9626 - val_loss: 0.4514 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.08926\n",
      "Epoch 33/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0965 - acc: 0.9678 - val_loss: 0.1213 - val_acc: 0.9535\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.08926\n",
      "Epoch 34/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0838 - acc: 0.9716 - val_loss: 0.1093 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.08926\n",
      "Epoch 35/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0814 - acc: 0.9716 - val_loss: 0.0943 - val_acc: 0.9625\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.08926\n",
      "Epoch 36/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0862 - acc: 0.9723 - val_loss: 0.0938 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.08926\n",
      "Epoch 37/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0808 - acc: 0.9691 - val_loss: 0.2469 - val_acc: 0.9159\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.08926\n",
      "Epoch 38/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0816 - acc: 0.9755 - val_loss: 0.1534 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.08926\n",
      "Epoch 39/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0923 - acc: 0.9697 - val_loss: 0.0863 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.08926 to 0.08633, saving model to AlexNet_based_check_point/39-0.0863.hdf5\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1552/1552 [==============================] - 6s 4ms/step - loss: 0.0816 - acc: 0.9742 - val_loss: 0.0910 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.08633\n",
      "Epoch 41/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0722 - acc: 0.9787 - val_loss: 0.1101 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.08633\n",
      "Epoch 42/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0805 - acc: 0.9704 - val_loss: 0.2858 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.08633\n",
      "Epoch 43/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0844 - acc: 0.9723 - val_loss: 0.0978 - val_acc: 0.9625\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.08633\n",
      "Epoch 44/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0780 - acc: 0.9762 - val_loss: 0.6832 - val_acc: 0.7913\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.08633\n",
      "Epoch 45/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0795 - acc: 0.9749 - val_loss: 0.0963 - val_acc: 0.9670\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.08633\n",
      "Epoch 46/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0757 - acc: 0.9729 - val_loss: 0.0894 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.08633\n",
      "Epoch 47/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0674 - acc: 0.9800 - val_loss: 1.2896 - val_acc: 0.6381\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.08633\n",
      "Epoch 48/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0763 - acc: 0.9736 - val_loss: 0.0985 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.08633\n",
      "Epoch 49/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0721 - acc: 0.9736 - val_loss: 0.1435 - val_acc: 0.9535\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.08633\n",
      "Epoch 50/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0731 - acc: 0.9749 - val_loss: 0.1268 - val_acc: 0.9489\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.08633\n",
      "Epoch 51/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0742 - acc: 0.9729 - val_loss: 0.2608 - val_acc: 0.9204\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.08633\n",
      "Epoch 52/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0641 - acc: 0.9787 - val_loss: 0.1400 - val_acc: 0.9505\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.08633\n",
      "Epoch 53/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0722 - acc: 0.9813 - val_loss: 3.0042 - val_acc: 0.3994\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.08633\n",
      "Epoch 54/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.1108 - acc: 0.9665 - val_loss: 0.1342 - val_acc: 0.9459\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.08633\n",
      "Epoch 55/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0578 - acc: 0.9807 - val_loss: 0.1000 - val_acc: 0.9670\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.08633\n",
      "Epoch 56/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0659 - acc: 0.9813 - val_loss: 0.0974 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.08633\n",
      "Epoch 57/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0609 - acc: 0.9832 - val_loss: 0.1096 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.08633\n",
      "Epoch 58/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0652 - acc: 0.9768 - val_loss: 0.1856 - val_acc: 0.9384\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.08633\n",
      "Epoch 59/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0612 - acc: 0.9794 - val_loss: 0.1300 - val_acc: 0.9565\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.08633\n",
      "Epoch 60/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0571 - acc: 0.9781 - val_loss: 0.1180 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.08633\n",
      "Epoch 61/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0582 - acc: 0.9807 - val_loss: 0.7508 - val_acc: 0.7733\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.08633\n",
      "Epoch 62/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0512 - acc: 0.9800 - val_loss: 0.3590 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.08633\n",
      "Epoch 63/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0791 - acc: 0.9729 - val_loss: 0.1454 - val_acc: 0.9550\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.08633\n",
      "Epoch 64/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0568 - acc: 0.9813 - val_loss: 0.1087 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.08633\n",
      "Epoch 65/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0561 - acc: 0.9768 - val_loss: 0.1281 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.08633\n",
      "Epoch 66/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0572 - acc: 0.9832 - val_loss: 0.1066 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.08633\n",
      "Epoch 67/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0560 - acc: 0.9807 - val_loss: 0.1866 - val_acc: 0.9414\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.08633\n",
      "Epoch 68/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0568 - acc: 0.9794 - val_loss: 0.3817 - val_acc: 0.8799\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.08633\n",
      "Epoch 69/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0550 - acc: 0.9820 - val_loss: 0.1116 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.08633\n",
      "Epoch 70/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0536 - acc: 0.9826 - val_loss: 0.1125 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.08633\n",
      "Epoch 71/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0574 - acc: 0.9839 - val_loss: 0.1833 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.08633\n",
      "Epoch 72/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0659 - acc: 0.9762 - val_loss: 0.1178 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.08633\n",
      "Epoch 73/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0487 - acc: 0.9813 - val_loss: 0.3238 - val_acc: 0.8979\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.08633\n",
      "Epoch 74/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0528 - acc: 0.9807 - val_loss: 0.1317 - val_acc: 0.9565\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.08633\n",
      "Epoch 75/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0543 - acc: 0.9794 - val_loss: 0.1197 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.08633\n",
      "Epoch 76/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0435 - acc: 0.9878 - val_loss: 0.1149 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.08633\n",
      "Epoch 77/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0363 - acc: 0.9871 - val_loss: 0.1437 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.08633\n",
      "Epoch 78/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0569 - acc: 0.9807 - val_loss: 0.1322 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.08633\n",
      "Epoch 79/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0527 - acc: 0.9826 - val_loss: 0.3115 - val_acc: 0.9219\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.08633\n",
      "Epoch 80/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0396 - acc: 0.9852 - val_loss: 0.5162 - val_acc: 0.9129\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.08633\n",
      "Epoch 81/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0430 - acc: 0.9813 - val_loss: 0.1149 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.08633\n",
      "Epoch 82/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0506 - acc: 0.9800 - val_loss: 0.1441 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.08633\n",
      "Epoch 83/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0356 - acc: 0.9878 - val_loss: 0.1350 - val_acc: 0.9610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00083: val_loss did not improve from 0.08633\n",
      "Epoch 84/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0283 - acc: 0.9903 - val_loss: 0.3570 - val_acc: 0.9234\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.08633\n",
      "Epoch 85/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0448 - acc: 0.9832 - val_loss: 0.2980 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.08633\n",
      "Epoch 86/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0608 - acc: 0.9800 - val_loss: 0.1224 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.08633\n",
      "Epoch 87/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0300 - acc: 0.9923 - val_loss: 1.1682 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.08633\n",
      "Epoch 88/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0459 - acc: 0.9845 - val_loss: 0.3174 - val_acc: 0.8979\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.08633\n",
      "Epoch 89/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0379 - acc: 0.9890 - val_loss: 0.1566 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.08633\n",
      "Epoch 90/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0481 - acc: 0.9800 - val_loss: 0.6332 - val_acc: 0.9069\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.08633\n",
      "Epoch 91/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0620 - acc: 0.9781 - val_loss: 0.1503 - val_acc: 0.9489\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.08633\n",
      "Epoch 92/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0515 - acc: 0.9865 - val_loss: 0.1697 - val_acc: 0.9414\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.08633\n",
      "Epoch 93/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0374 - acc: 0.9884 - val_loss: 0.1767 - val_acc: 0.9489\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.08633\n",
      "Epoch 94/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0319 - acc: 0.9897 - val_loss: 0.1276 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.08633\n",
      "Epoch 95/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0241 - acc: 0.9916 - val_loss: 0.1424 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.08633\n",
      "Epoch 96/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0533 - acc: 0.9865 - val_loss: 0.2444 - val_acc: 0.9429\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.08633\n",
      "Epoch 97/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0435 - acc: 0.9865 - val_loss: 0.3766 - val_acc: 0.8799\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.08633\n",
      "Epoch 98/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0580 - acc: 0.9794 - val_loss: 1.2379 - val_acc: 0.6877\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.08633\n",
      "Epoch 99/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0371 - acc: 0.9890 - val_loss: 0.1650 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.08633\n",
      "Epoch 100/100\n",
      "1552/1552 [==============================] - 4s 3ms/step - loss: 0.0243 - acc: 0.9903 - val_loss: 2.4683 - val_acc: 0.4925\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.08633\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, batch_size=64, epochs=100, validation_split=0.3, shuffle=True, callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model(model_path+'39-0.0863.hdf5',custom_objects={'<lambda>': ParametricSoftplus(0.2, 0.5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DefsBoWAFKdi",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093/1093 [==============================] - 1s 1ms/step\n",
      "Loss: 0.12353629554125113 Accuracy: 0.9560841720036597\n"
     ]
    }
   ],
   "source": [
    "[loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "print('Loss:', loss, 'Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9663865546218486\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "test_f1_score = f1_score(y_test, pred > 0.5)\n",
    "print('F1 Score:', test_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801 ms ± 14.5 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 5 -r 5 model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "brains_on_beats_model_test",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

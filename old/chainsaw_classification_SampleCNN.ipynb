{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fWomgC-kF5f"
   },
   "source": [
    "**Architecture **\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=12JomC2IswVbNGdE0IIvPpUk8vPjP-MBQ\"  alt=\"artchtecture\">\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uN7hQRZsDbgI"
   },
   "source": [
    "(1) Importing dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3lPxjI5BDAkX",
    "outputId": "88280284-3c51-485b-adfa-4c428507fb92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,\\\n",
    "                         Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "import random\n",
    "random.seed(13)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "py5KMVLnDZsC"
   },
   "source": [
    "(2) Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/data/private/SU/bbchip13/chainsaw_classification/data/'\n",
    "train_dir = base_dir+'train/'\n",
    "val_dir = base_dir+'val/'\n",
    "test_dir = base_dir+'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wavs(filenames):\n",
    "    return np.asarray([librosa.load(filename)[0] for filename in tqdm(filenames)])\n",
    "\n",
    "### If you have lack of memory, Use this\n",
    "#     wav = librosa.load(filenames[0])\n",
    "#     wavs = np.zeros( (len(filenames), wav.shape[0]) )\n",
    "#     for i, filename in enumerate(filenames):\n",
    "#         wavs[i][:] = librosa.load(filename)[:]\n",
    "#     return wavs\n",
    "    \n",
    "def find_y_by_filename(filename, y_dict):\n",
    "    basename = os.path.basename(filename)\n",
    "    y = y_dict[basename]\n",
    "    return y\n",
    "\n",
    "def make_y_by_filenames(filenames, y_dict):\n",
    "    return np.asarray([find_y_by_filename(filename, y_dict) \n",
    "                           for filename in filenames])\n",
    "\n",
    "def make_xy_data(filenames, y_dict):\n",
    "    x_train = load_wavs(filenames)\n",
    "    x_train = np.reshape(x_train, (*x_train.shape, 1))\n",
    "    y_train = make_y_by_filenames(filenames, y_dict)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make Y data\n",
    "annotations_filename = 'data_annotations.csv'\n",
    "df = pd.read_csv(annotations_filename)\n",
    "y_dict = {filename:int(label) for _, filename, label, _ in df.itertuples()}\n",
    "# y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make train data.......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c038e0f48f43f7956441b86d11eec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1543), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Make validation data.......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58807e0f87804692b97557fae38a0cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=714), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(1543, 110250, 1) (1543,) (714, 110250, 1) (714,)\n"
     ]
    }
   ],
   "source": [
    "print('Make train data.......')\n",
    "x_train_wav_filenames = [train_dir+filename for filename in os.listdir(train_dir)\n",
    "                            if filename.endswith('.wav')]#[:10]\n",
    "x_train, y_train = make_xy_data(x_train_wav_filenames, y_dict)\n",
    "\n",
    "print('Make validation data.......')\n",
    "x_val_wav_filenames = [val_dir+filename for filename in os.listdir(val_dir)\n",
    "                            if filename.endswith('.wav')]#[:2]\n",
    "x_val, y_val = make_xy_data(x_val_wav_filenames, y_dict)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12cS85jvDnfS"
   },
   "source": [
    "(3) Create a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "colab_type": "code",
    "id": "fs8Heys2Dm30",
    "outputId": "bad14ede-be9c-4a2f-d052-9d9a29a5e437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 36750, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 36750, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 36750, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 36750, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 36750, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 36750, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 12250, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 12250, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12250, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 12250, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4083, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 4083, 256)         98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4083, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4083, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1361, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1361, 256)         196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1361, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1361, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 453, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 453, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 453, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 453, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 151, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 151, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 151, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 151, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 50, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 50, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 5, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,585,921\n",
      "Trainable params: 1,581,057\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv1D (kernel_size=3, filters=128, strides=3, padding='valid',\n",
    "                  kernel_initializer='he_uniform', input_shape=x_train.shape[1:]))                  \n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv1D (kernel_size=3, filters=128, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 3\n",
    "model.add(Conv1D (kernel_size=3, filters=128, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 4\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 5\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 6\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 7\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 8\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 9\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 10\n",
    "model.add(Conv1D (kernel_size=3, filters=512, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 11\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Layer 12\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLxfqHNxDuJq"
   },
   "source": [
    "(4) Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jPB8IbZDxeJ"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUsuRj-7Dzxx"
   },
   "source": [
    "(5) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'SampleCNN_check_point/'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "model_filename = model_path+'{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "ZUVV71K2D2tZ",
    "outputId": "7a454152-003e-4615-acd8-cfdb60ef8170",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Train on 1543 samples, validate on 714 samples\n",
      "Epoch 1/10000\n",
      "1543/1543 [==============================] - 33s 22ms/step - loss: 0.3428 - acc: 0.8691 - val_loss: 0.2838 - val_acc: 0.9146\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.28384, saving model to SampleCNN_check_point/01-0.2838.hdf5\n",
      "Epoch 2/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.1758 - acc: 0.9417 - val_loss: 0.6050 - val_acc: 0.8151\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.28384\n",
      "Epoch 3/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.1505 - acc: 0.9475 - val_loss: 0.1771 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.28384 to 0.17711, saving model to SampleCNN_check_point/03-0.1771.hdf5\n",
      "Epoch 4/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.1221 - acc: 0.9514 - val_loss: 0.1434 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17711 to 0.14339, saving model to SampleCNN_check_point/04-0.1434.hdf5\n",
      "Epoch 5/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.1014 - acc: 0.9682 - val_loss: 0.2789 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.14339\n",
      "Epoch 6/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0908 - acc: 0.9689 - val_loss: 0.1336 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.14339 to 0.13359, saving model to SampleCNN_check_point/06-0.1336.hdf5\n",
      "Epoch 7/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0872 - acc: 0.9689 - val_loss: 0.0999 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.13359 to 0.09990, saving model to SampleCNN_check_point/07-0.0999.hdf5\n",
      "Epoch 8/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0648 - acc: 0.9721 - val_loss: 0.1844 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09990\n",
      "Epoch 9/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0714 - acc: 0.9682 - val_loss: 0.1522 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09990\n",
      "Epoch 10/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0550 - acc: 0.9806 - val_loss: 0.1346 - val_acc: 0.9636\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.09990\n",
      "Epoch 11/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0615 - acc: 0.9773 - val_loss: 0.1334 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.09990\n",
      "Epoch 12/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0470 - acc: 0.9812 - val_loss: 0.1656 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.09990\n",
      "Epoch 13/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0581 - acc: 0.9812 - val_loss: 0.1447 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.09990\n",
      "Epoch 14/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0351 - acc: 0.9896 - val_loss: 0.1405 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09990\n",
      "Epoch 15/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0331 - acc: 0.9870 - val_loss: 0.1783 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.09990\n",
      "Epoch 16/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0450 - acc: 0.9851 - val_loss: 0.3937 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.09990\n",
      "Epoch 17/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.1777 - acc: 0.9423 - val_loss: 0.6866 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.09990\n",
      "Epoch 18/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.1520 - acc: 0.9475 - val_loss: 0.2383 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.09990\n",
      "Epoch 19/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0856 - acc: 0.9682 - val_loss: 0.1226 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09990\n",
      "Epoch 20/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0865 - acc: 0.9715 - val_loss: 0.1444 - val_acc: 0.9622\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09990\n",
      "Epoch 21/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0925 - acc: 0.9663 - val_loss: 0.1518 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.09990\n",
      "Epoch 22/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0932 - acc: 0.9657 - val_loss: 0.2154 - val_acc: 0.9342\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.09990\n",
      "Epoch 23/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.1046 - acc: 0.9598 - val_loss: 0.3383 - val_acc: 0.9272\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.09990\n",
      "Epoch 24/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0746 - acc: 0.9741 - val_loss: 0.1880 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.09990\n",
      "Epoch 25/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0583 - acc: 0.9806 - val_loss: 0.1732 - val_acc: 0.9384\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.09990\n",
      "Epoch 26/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0951 - acc: 0.9650 - val_loss: 0.2462 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.09990\n",
      "Epoch 27/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0621 - acc: 0.9767 - val_loss: 0.0905 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.09990 to 0.09054, saving model to SampleCNN_check_point/27-0.0905.hdf5\n",
      "Epoch 28/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0810 - acc: 0.9734 - val_loss: 0.1242 - val_acc: 0.9566\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.09054\n",
      "Epoch 29/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.1395 - acc: 0.9540 - val_loss: 0.2044 - val_acc: 0.9342\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.09054\n",
      "Epoch 30/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0933 - acc: 0.9657 - val_loss: 0.1737 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.09054\n",
      "Epoch 31/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0904 - acc: 0.9702 - val_loss: 0.1703 - val_acc: 0.9566\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.09054\n",
      "Epoch 32/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0753 - acc: 0.9767 - val_loss: 0.1326 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.09054\n",
      "Epoch 33/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0518 - acc: 0.9825 - val_loss: 0.0904 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.09054 to 0.09044, saving model to SampleCNN_check_point/33-0.0904.hdf5\n",
      "Epoch 34/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0510 - acc: 0.9844 - val_loss: 0.0749 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.09044 to 0.07488, saving model to SampleCNN_check_point/34-0.0749.hdf5\n",
      "Epoch 35/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0654 - acc: 0.9721 - val_loss: 0.1241 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.07488\n",
      "Epoch 36/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0659 - acc: 0.9767 - val_loss: 1.2655 - val_acc: 0.6345\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.07488\n",
      "Epoch 37/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.1119 - acc: 0.9501 - val_loss: 0.8780 - val_acc: 0.8123\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.07488\n",
      "Epoch 38/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0504 - acc: 0.9806 - val_loss: 0.8761 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.07488\n",
      "Epoch 39/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0987 - acc: 0.9669 - val_loss: 0.1169 - val_acc: 0.9594\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.07488\n",
      "Epoch 40/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.1094 - acc: 0.9618 - val_loss: 0.0941 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.07488\n",
      "Epoch 41/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0645 - acc: 0.9793 - val_loss: 0.0830 - val_acc: 0.9636\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.07488\n",
      "Epoch 42/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0577 - acc: 0.9806 - val_loss: 0.0834 - val_acc: 0.9664\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.07488\n",
      "Epoch 43/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0487 - acc: 0.9825 - val_loss: 0.1904 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.07488\n",
      "Epoch 44/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0555 - acc: 0.9831 - val_loss: 0.0890 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.07488\n",
      "Epoch 45/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0483 - acc: 0.9864 - val_loss: 0.0720 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.07488 to 0.07202, saving model to SampleCNN_check_point/45-0.0720.hdf5\n",
      "Epoch 46/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0445 - acc: 0.9838 - val_loss: 0.0888 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.07202\n",
      "Epoch 47/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0481 - acc: 0.9812 - val_loss: 0.0913 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.07202\n",
      "Epoch 48/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0394 - acc: 0.9877 - val_loss: 0.1576 - val_acc: 0.9622\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.07202\n",
      "Epoch 49/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0523 - acc: 0.9812 - val_loss: 0.0825 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.07202\n",
      "Epoch 50/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0539 - acc: 0.9825 - val_loss: 0.1621 - val_acc: 0.9594\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.07202\n",
      "Epoch 51/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0880 - acc: 0.9708 - val_loss: 0.2060 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.07202\n",
      "Epoch 52/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0568 - acc: 0.9812 - val_loss: 0.2129 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.07202\n",
      "Epoch 53/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0693 - acc: 0.9715 - val_loss: 0.2718 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.07202\n",
      "Epoch 54/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0516 - acc: 0.9851 - val_loss: 0.2029 - val_acc: 0.9622\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.07202\n",
      "Epoch 55/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0298 - acc: 0.9903 - val_loss: 0.1022 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.07202\n",
      "Epoch 56/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0323 - acc: 0.9903 - val_loss: 0.1150 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.07202\n",
      "Epoch 57/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0359 - acc: 0.9877 - val_loss: 0.2614 - val_acc: 0.9020\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.07202\n",
      "Epoch 58/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0934 - acc: 0.9702 - val_loss: 0.1301 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.07202\n",
      "Epoch 59/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0384 - acc: 0.9903 - val_loss: 0.1170 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.07202\n",
      "Epoch 60/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0321 - acc: 0.9877 - val_loss: 0.1716 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.07202\n",
      "Epoch 61/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0468 - acc: 0.9812 - val_loss: 0.3396 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07202\n",
      "Epoch 62/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0489 - acc: 0.9780 - val_loss: 0.1983 - val_acc: 0.9636\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07202\n",
      "Epoch 63/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0215 - acc: 0.9935 - val_loss: 0.1199 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.07202\n",
      "Epoch 64/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0244 - acc: 0.9916 - val_loss: 0.1305 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.07202\n",
      "Epoch 65/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0507 - acc: 0.9819 - val_loss: 0.3865 - val_acc: 0.8599\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.07202\n",
      "Epoch 66/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.1248 - acc: 0.9592 - val_loss: 0.2495 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.07202\n",
      "Epoch 67/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0301 - acc: 0.9896 - val_loss: 0.1323 - val_acc: 0.9664\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.07202\n",
      "Epoch 68/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0297 - acc: 0.9883 - val_loss: 0.0892 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.07202\n",
      "Epoch 69/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0211 - acc: 0.9903 - val_loss: 0.0911 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.07202\n",
      "Epoch 70/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0177 - acc: 0.9942 - val_loss: 0.0904 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.07202\n",
      "Epoch 71/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0343 - acc: 0.9877 - val_loss: 0.1293 - val_acc: 0.9664\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.07202\n",
      "Epoch 72/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0474 - acc: 0.9922 - val_loss: 0.1490 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.07202\n",
      "Epoch 73/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.1031 - acc: 0.9702 - val_loss: 0.1321 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.07202\n",
      "Epoch 74/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0536 - acc: 0.9812 - val_loss: 0.1419 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.07202\n",
      "Epoch 75/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0360 - acc: 0.9877 - val_loss: 0.1540 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.07202\n",
      "Epoch 76/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0361 - acc: 0.9857 - val_loss: 0.1160 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.07202\n",
      "Epoch 77/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0252 - acc: 0.9896 - val_loss: 0.0787 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.07202\n",
      "Epoch 78/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0137 - acc: 0.9955 - val_loss: 0.0966 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.07202\n",
      "Epoch 79/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0206 - acc: 0.9935 - val_loss: 0.0794 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.07202\n",
      "Epoch 80/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0212 - acc: 0.9935 - val_loss: 0.1493 - val_acc: 0.9636\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.07202\n",
      "Epoch 81/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0752 - acc: 0.9825 - val_loss: 0.2794 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.07202\n",
      "Epoch 82/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0529 - acc: 0.9857 - val_loss: 0.1902 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.07202\n",
      "Epoch 83/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0254 - acc: 0.9916 - val_loss: 0.1172 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.07202\n",
      "Epoch 84/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0213 - acc: 0.9922 - val_loss: 0.1204 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.07202\n",
      "Epoch 85/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0158 - acc: 0.9948 - val_loss: 0.0714 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.07202 to 0.07137, saving model to SampleCNN_check_point/85-0.0714.hdf5\n",
      "Epoch 86/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0142 - acc: 0.9935 - val_loss: 0.0808 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.07137\n",
      "Epoch 87/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0298 - acc: 0.9890 - val_loss: 0.1141 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.07137\n",
      "Epoch 88/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0230 - acc: 0.9935 - val_loss: 0.0951 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.07137\n",
      "Epoch 89/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0347 - acc: 0.9851 - val_loss: 0.1635 - val_acc: 0.9594\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.07137\n",
      "Epoch 90/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0636 - acc: 0.9786 - val_loss: 0.1422 - val_acc: 0.9594\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.07137\n",
      "Epoch 91/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0555 - acc: 0.9786 - val_loss: 0.1284 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.07137\n",
      "Epoch 92/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0807 - acc: 0.9793 - val_loss: 0.1616 - val_acc: 0.9566\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.07137\n",
      "Epoch 93/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0800 - acc: 0.9734 - val_loss: 0.2218 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.07137\n",
      "Epoch 94/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0294 - acc: 0.9922 - val_loss: 0.1330 - val_acc: 0.9594\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.07137\n",
      "Epoch 95/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0206 - acc: 0.9929 - val_loss: 0.1126 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.07137\n",
      "Epoch 96/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0194 - acc: 0.9929 - val_loss: 0.1221 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.07137\n",
      "Epoch 97/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0172 - acc: 0.9942 - val_loss: 0.1309 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.07137\n",
      "Epoch 98/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0107 - acc: 0.9961 - val_loss: 0.0801 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.07137\n",
      "Epoch 99/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0172 - acc: 0.9955 - val_loss: 0.1230 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.07137\n",
      "Epoch 100/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0114 - acc: 0.9981 - val_loss: 0.1132 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.07137\n",
      "Epoch 101/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0169 - acc: 0.9935 - val_loss: 0.0950 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.07137\n",
      "Epoch 102/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0296 - acc: 0.9916 - val_loss: 0.1047 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.07137\n",
      "Epoch 103/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0252 - acc: 0.9909 - val_loss: 0.1180 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.07137\n",
      "Epoch 104/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0274 - acc: 0.9896 - val_loss: 0.1701 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.07137\n",
      "Epoch 105/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0175 - acc: 0.9968 - val_loss: 0.3226 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.07137\n",
      "Epoch 106/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0852 - acc: 0.9747 - val_loss: 0.5404 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.07137\n",
      "Epoch 107/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0466 - acc: 0.9825 - val_loss: 0.1673 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.07137\n",
      "Epoch 108/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0192 - acc: 0.9922 - val_loss: 0.2160 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.07137\n",
      "Epoch 109/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0169 - acc: 0.9955 - val_loss: 0.1419 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.07137\n",
      "Epoch 110/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0119 - acc: 0.9955 - val_loss: 0.1486 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.07137\n",
      "Epoch 111/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0108 - acc: 0.9942 - val_loss: 0.1951 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.07137\n",
      "Epoch 112/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0209 - acc: 0.9935 - val_loss: 0.1706 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.07137\n",
      "Epoch 113/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0074 - acc: 0.9987 - val_loss: 0.1804 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.07137\n",
      "Epoch 114/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0119 - acc: 0.9961 - val_loss: 0.1494 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.07137\n",
      "Epoch 115/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0324 - acc: 0.9916 - val_loss: 0.1334 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.07137\n",
      "Epoch 116/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0200 - acc: 0.9922 - val_loss: 0.0978 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.07137\n",
      "Epoch 117/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0105 - acc: 0.9961 - val_loss: 0.1205 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.07137\n",
      "Epoch 118/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0091 - acc: 0.9961 - val_loss: 0.0997 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.07137\n",
      "Epoch 119/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0133 - acc: 0.9955 - val_loss: 0.1313 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.07137\n",
      "Epoch 120/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0096 - acc: 0.9955 - val_loss: 0.1587 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.07137\n",
      "Epoch 121/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0093 - acc: 0.9968 - val_loss: 0.1573 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.07137\n",
      "Epoch 122/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0049 - acc: 0.9974 - val_loss: 0.1101 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.07137\n",
      "Epoch 123/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0136 - acc: 0.9955 - val_loss: 0.1378 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.07137\n",
      "Epoch 124/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0165 - acc: 0.9955 - val_loss: 0.1502 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.07137\n",
      "Epoch 125/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0066 - acc: 0.9968 - val_loss: 0.1954 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.07137\n",
      "Epoch 126/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0218 - acc: 0.9903 - val_loss: 0.1844 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.07137\n",
      "Epoch 127/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0134 - acc: 0.9974 - val_loss: 0.1537 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.07137\n",
      "Epoch 128/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0085 - acc: 0.9987 - val_loss: 0.1548 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.07137\n",
      "Epoch 129/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0137 - acc: 0.9961 - val_loss: 0.1456 - val_acc: 0.9594\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.07137\n",
      "Epoch 130/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0067 - acc: 0.9981 - val_loss: 0.1401 - val_acc: 0.9664\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.07137\n",
      "Epoch 131/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0096 - acc: 0.9955 - val_loss: 0.1215 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.07137\n",
      "Epoch 132/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0208 - acc: 0.9955 - val_loss: 0.1705 - val_acc: 0.9664\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.07137\n",
      "Epoch 133/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0540 - acc: 0.9851 - val_loss: 0.1887 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.07137\n",
      "Epoch 134/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0294 - acc: 0.9916 - val_loss: 0.2646 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.07137\n",
      "Epoch 135/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0674 - acc: 0.9760 - val_loss: 0.4157 - val_acc: 0.9356\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.07137\n",
      "Epoch 136/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0206 - acc: 0.9935 - val_loss: 0.3298 - val_acc: 0.9594\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.07137\n",
      "Epoch 137/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0147 - acc: 0.9935 - val_loss: 0.2602 - val_acc: 0.9636\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.07137\n",
      "Epoch 138/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0201 - acc: 0.9942 - val_loss: 0.2022 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.07137\n",
      "Epoch 139/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0583 - acc: 0.9754 - val_loss: 0.1014 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.07137\n",
      "Epoch 140/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0204 - acc: 0.9916 - val_loss: 0.1567 - val_acc: 0.9664\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.07137\n",
      "Epoch 141/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0277 - acc: 0.9929 - val_loss: 0.0973 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.07137\n",
      "Epoch 142/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0104 - acc: 0.9961 - val_loss: 0.0877 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.07137\n",
      "Epoch 143/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0227 - acc: 0.9916 - val_loss: 0.0980 - val_acc: 0.9762\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.07137\n",
      "Epoch 144/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0208 - acc: 0.9942 - val_loss: 0.0798 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.07137\n",
      "Epoch 145/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0162 - acc: 0.9922 - val_loss: 0.0935 - val_acc: 0.9762\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.07137\n",
      "Epoch 146/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0219 - acc: 0.9955 - val_loss: 0.1086 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.07137\n",
      "Epoch 147/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0178 - acc: 0.9955 - val_loss: 0.0989 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.07137\n",
      "Epoch 148/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0069 - acc: 0.9981 - val_loss: 0.1032 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.07137\n",
      "Epoch 149/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0157 - acc: 0.9935 - val_loss: 0.1016 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.07137\n",
      "Epoch 150/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0254 - acc: 0.9922 - val_loss: 0.1030 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.07137\n",
      "Epoch 151/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0203 - acc: 0.9955 - val_loss: 0.0972 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.07137\n",
      "Epoch 152/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0089 - acc: 0.9987 - val_loss: 0.0993 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.07137\n",
      "Epoch 153/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0152 - acc: 0.9961 - val_loss: 0.1036 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.07137\n",
      "Epoch 154/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0067 - acc: 0.9974 - val_loss: 0.0985 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.07137\n",
      "Epoch 155/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0266 - acc: 0.9916 - val_loss: 0.2231 - val_acc: 0.9412\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.07137\n",
      "Epoch 156/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0787 - acc: 0.9702 - val_loss: 0.2922 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.07137\n",
      "Epoch 157/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0334 - acc: 0.9890 - val_loss: 0.1917 - val_acc: 0.9636\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.07137\n",
      "Epoch 158/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0318 - acc: 0.9903 - val_loss: 0.1570 - val_acc: 0.9636\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.07137\n",
      "Epoch 159/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0348 - acc: 0.9857 - val_loss: 0.1454 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.07137\n",
      "Epoch 160/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0153 - acc: 0.9942 - val_loss: 0.1439 - val_acc: 0.9622\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.07137\n",
      "Epoch 161/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0404 - acc: 0.9838 - val_loss: 0.1359 - val_acc: 0.9664\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.07137\n",
      "Epoch 162/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0197 - acc: 0.9929 - val_loss: 0.1041 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.07137\n",
      "Epoch 163/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0337 - acc: 0.9903 - val_loss: 0.1163 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.07137\n",
      "Epoch 164/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0073 - acc: 0.9974 - val_loss: 0.1253 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.07137\n",
      "Epoch 165/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0156 - acc: 0.9987 - val_loss: 0.0985 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.07137\n",
      "Epoch 166/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0415 - acc: 0.9864 - val_loss: 0.1188 - val_acc: 0.9580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00166: val_loss did not improve from 0.07137\n",
      "Epoch 167/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0147 - acc: 0.9961 - val_loss: 0.1243 - val_acc: 0.9636\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.07137\n",
      "Epoch 168/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0148 - acc: 0.9942 - val_loss: 0.0984 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.07137\n",
      "Epoch 169/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0121 - acc: 0.9948 - val_loss: 0.1298 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.07137\n",
      "Epoch 170/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0352 - acc: 0.9877 - val_loss: 0.1171 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.07137\n",
      "Epoch 171/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0607 - acc: 0.9773 - val_loss: 0.1869 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.07137\n",
      "Epoch 172/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0274 - acc: 0.9896 - val_loss: 0.1095 - val_acc: 0.9762\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.07137\n",
      "Epoch 173/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0176 - acc: 0.9955 - val_loss: 0.1417 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.07137\n",
      "Epoch 174/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0138 - acc: 0.9961 - val_loss: 0.1077 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.07137\n",
      "Epoch 175/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0083 - acc: 0.9968 - val_loss: 0.1442 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.07137\n",
      "Epoch 176/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0074 - acc: 0.9981 - val_loss: 0.1281 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.07137\n",
      "Epoch 177/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0074 - acc: 0.9974 - val_loss: 0.1421 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.07137\n",
      "Epoch 178/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0207 - acc: 0.9935 - val_loss: 0.1755 - val_acc: 0.9636\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.07137\n",
      "Epoch 179/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0203 - acc: 0.9942 - val_loss: 0.3491 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.07137\n",
      "Epoch 180/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0136 - acc: 0.9955 - val_loss: 0.2923 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.07137\n",
      "Epoch 181/10000\n",
      "1543/1543 [==============================] - 27s 18ms/step - loss: 0.0145 - acc: 0.9948 - val_loss: 0.1893 - val_acc: 0.9664\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.07137\n",
      "Epoch 182/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0338 - acc: 0.9877 - val_loss: 0.2844 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.07137\n",
      "Epoch 183/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0206 - acc: 0.9942 - val_loss: 0.1299 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.07137\n",
      "Epoch 184/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0079 - acc: 0.9981 - val_loss: 0.1537 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.07137\n",
      "Epoch 185/10000\n",
      "1543/1543 [==============================] - 27s 17ms/step - loss: 0.0066 - acc: 0.9987 - val_loss: 0.1258 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.07137\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, batch_size=64, epochs=10000, validation_data=[x_val, y_val], \n",
    "                 shuffle=True, callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make test data.......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fba1dc0cf74a6ea3277bf05922c21e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1054), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Make test data.......')\n",
    "x_test_wav_filenames = [test_dir+filename for filename in os.listdir(test_dir)\n",
    "                            if filename.endswith('.wav')]#[:2]\n",
    "x_test, y_test = make_xy_data(x_test_wav_filenames, y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = load_model(model_path+'85-0.0714.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DefsBoWAFKdi",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054/1054 [==============================] - 7s 7ms/step\n",
      "Loss: 0.0547122646731093 Accuracy: 0.9829222011385199\n"
     ]
    }
   ],
   "source": [
    "[loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "print('Loss:', loss, 'Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9858044164037855\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "test_f1_score = f1_score(y_test, pred > 0.5)\n",
    "print('F1 Score:', test_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.94 s ± 9.97 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 5 -r 5 model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "brains_on_beats_model_test",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

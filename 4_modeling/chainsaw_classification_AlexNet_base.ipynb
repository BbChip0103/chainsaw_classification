{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fWomgC-kF5f"
   },
   "source": [
    "**Architecture **\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=12JomC2IswVbNGdE0IIvPpUk8vPjP-MBQ\"  alt=\"artchtecture\">\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uN7hQRZsDbgI"
   },
   "source": [
    "(1) Importing dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3lPxjI5BDAkX",
    "outputId": "88280284-3c51-485b-adfa-4c428507fb92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,\\\n",
    "                         Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "import random\n",
    "random.seed(13)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "py5KMVLnDZsC"
   },
   "source": [
    "(2) Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/data/private/SU/bbchip13/chainsaw_classification/data/'\n",
    "train_dir = base_dir+'train/'\n",
    "val_dir = base_dir+'val/'\n",
    "test_dir = base_dir+'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wavs(filenames):\n",
    "    return np.asarray([librosa.load(filename)[0] for filename in tqdm(filenames)])\n",
    "\n",
    "### If you have lack of memory, Use this\n",
    "#     wav = librosa.load(filenames[0])\n",
    "#     wavs = np.zeros( (len(filenames), wav.shape[0]) )\n",
    "#     for i, filename in enumerate(filenames):\n",
    "#         wavs[i][:] = librosa.load(filename)[:]\n",
    "#     return wavs\n",
    "    \n",
    "def find_y_by_filename(filename, y_dict):\n",
    "    basename = os.path.basename(filename)\n",
    "    y = y_dict[basename]\n",
    "    return y\n",
    "\n",
    "def make_y_by_filenames(filenames, y_dict):\n",
    "    return np.asarray([find_y_by_filename(filename, y_dict) \n",
    "                           for filename in filenames])\n",
    "\n",
    "def make_xy_data(filenames, y_dict):\n",
    "    x_train = load_wavs(filenames)\n",
    "    x_train = np.reshape(x_train, (*x_train.shape, 1))\n",
    "    y_train = make_y_by_filenames(filenames, y_dict)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make Y data\n",
    "annotations_filename = 'data_annotations.csv'\n",
    "df = pd.read_csv(annotations_filename)\n",
    "y_dict = {filename:int(label) for _, filename, label, _ in df.itertuples()}\n",
    "# y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make train data.......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d143a49732774979bbad5b28433a0614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1543), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Make validation data.......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8132fbd1465345fb8cdba81b77a4541e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=714), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(1543, 110250, 1) (1543,) (714, 110250, 1) (714,)\n"
     ]
    }
   ],
   "source": [
    "print('Make train data.......')\n",
    "x_train_wav_filenames = [train_dir+filename for filename in os.listdir(train_dir)\n",
    "                            if filename.endswith('.wav')]#[:1000]\n",
    "x_train, y_train = make_xy_data(x_train_wav_filenames, y_dict)\n",
    "\n",
    "print('Make validation data.......')\n",
    "x_val_wav_filenames = [val_dir+filename for filename in os.listdir(val_dir)\n",
    "                            if filename.endswith('.wav')]#[:200]\n",
    "x_val, y_val = make_xy_data(x_val_wav_filenames, y_dict)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12cS85jvDnfS"
   },
   "source": [
    "(3) Create a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4GDedqMJcJYr"
   },
   "outputs": [],
   "source": [
    "### Define Parametric Softplus\n",
    "\n",
    "# alpha * log(1 + exp(beta * x))\n",
    "def ParametricSoftplus(alpha=0.2, beta=5.0):\n",
    "  return lambda x: alpha * keras.activations.softplus(beta * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "colab_type": "code",
    "id": "fs8Heys2Dm30",
    "outputId": "bad14ede-be9c-4a2f-d052-9d9a29a5e437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 6891, 48)          5856      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6891, 48)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 6891, 48)          192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1723, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1723, 128)         153728    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1723, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1723, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 431, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 431, 192)          221376    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 431, 192)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 431, 192)          331968    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 431, 192)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 431, 128)          221312    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 431, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 108, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 13824)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              56627200  \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 4097      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 74,347,553\n",
      "Trainable params: 74,347,201\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "# 1st Convolutional Layer (conv1)\n",
    "model.add(Conv1D (kernel_size=121, filters=48, strides=16, padding='same',\n",
    "#                   input_shape=x_train.shape[1:]))\n",
    "                  input_shape=(110250, 1)))\n",
    "\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Pooling (pool1)\n",
    "model.add(MaxPooling1D(pool_size=9, strides=4, padding='same'))\n",
    "\n",
    "# 2nd Convolutional Layer (conv2)\n",
    "model.add(Conv1D (kernel_size=25, filters=128, padding='same'))\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Pooling (pool2)\n",
    "model.add(MaxPooling1D(pool_size=9, strides=4, padding='same'))\n",
    "\n",
    "# 3rd Convolutional Layer (conv3)\n",
    "model.add(Conv1D (kernel_size=9, filters=192, padding='same'))\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "\n",
    "# 4rd Convolutional Layer (conv4)\n",
    "model.add(Conv1D (kernel_size=9, filters=192, padding='same'))\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "\n",
    "# 5rd Convolutional Layer (conv5)\n",
    "model.add(Conv1D (kernel_size=9, filters=128, padding='same'))\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "\n",
    "# Pooling (pool5)\n",
    "model.add(MaxPooling1D(pool_size=9, strides=4, padding='same'))\n",
    "\n",
    "# 1st Dense Layer (full6)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "model.add(Dropout(0.5)) # Drop-out value is not specified in the paper\n",
    "\n",
    "# 2nd Dense Layer (full7)\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation(ParametricSoftplus(alpha=0.2, beta=0.5)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output Layer (full8)\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLxfqHNxDuJq"
   },
   "source": [
    "(4) Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jPB8IbZDxeJ"
   },
   "outputs": [],
   "source": [
    "adam_with_params = keras.optimizers.Adam(lr=0.0002, beta_1=0.1, beta_2=0.999, \n",
    "                                         epsilon=1e-8)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam_with_params,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUsuRj-7Dzxx"
   },
   "source": [
    "(5) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'AlexNet_based_check_point/'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "model_filename = model_path+'{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "ZUVV71K2D2tZ",
    "outputId": "7a454152-003e-4615-acd8-cfdb60ef8170",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1543 samples, validate on 714 samples\n",
      "Epoch 1/10000\n",
      "1543/1543 [==============================] - 9s 6ms/step - loss: 0.6496 - acc: 0.6507 - val_loss: 0.8408 - val_acc: 0.6218\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.84076, saving model to AlexNet_based_check_point/01-0.8408.hdf5\n",
      "Epoch 2/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.4826 - acc: 0.7732 - val_loss: 0.7537 - val_acc: 0.7395\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.84076 to 0.75373, saving model to AlexNet_based_check_point/02-0.7537.hdf5\n",
      "Epoch 3/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.2578 - acc: 0.9151 - val_loss: 0.3216 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.75373 to 0.32160, saving model to AlexNet_based_check_point/03-0.3216.hdf5\n",
      "Epoch 4/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.2076 - acc: 0.9255 - val_loss: 0.2899 - val_acc: 0.9048\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32160 to 0.28991, saving model to AlexNet_based_check_point/04-0.2899.hdf5\n",
      "Epoch 5/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1873 - acc: 0.9339 - val_loss: 0.2187 - val_acc: 0.9272\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.28991 to 0.21874, saving model to AlexNet_based_check_point/05-0.2187.hdf5\n",
      "Epoch 6/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1840 - acc: 0.9391 - val_loss: 0.2627 - val_acc: 0.9090\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.21874\n",
      "Epoch 7/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1801 - acc: 0.9378 - val_loss: 0.2033 - val_acc: 0.9342\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.21874 to 0.20328, saving model to AlexNet_based_check_point/07-0.2033.hdf5\n",
      "Epoch 8/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1613 - acc: 0.9417 - val_loss: 0.1968 - val_acc: 0.9342\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.20328 to 0.19679, saving model to AlexNet_based_check_point/08-0.1968.hdf5\n",
      "Epoch 9/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1540 - acc: 0.9488 - val_loss: 0.3730 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.19679\n",
      "Epoch 10/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1703 - acc: 0.9449 - val_loss: 0.2105 - val_acc: 0.9300\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.19679\n",
      "Epoch 11/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1539 - acc: 0.9443 - val_loss: 0.1918 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.19679 to 0.19185, saving model to AlexNet_based_check_point/11-0.1918.hdf5\n",
      "Epoch 12/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1331 - acc: 0.9527 - val_loss: 0.2434 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.19185\n",
      "Epoch 13/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1434 - acc: 0.9462 - val_loss: 0.2345 - val_acc: 0.9188\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.19185\n",
      "Epoch 14/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1386 - acc: 0.9514 - val_loss: 0.1997 - val_acc: 0.9328\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.19185\n",
      "Epoch 15/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1098 - acc: 0.9572 - val_loss: 0.2341 - val_acc: 0.9272\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.19185\n",
      "Epoch 16/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1217 - acc: 0.9559 - val_loss: 0.4932 - val_acc: 0.7745\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.19185\n",
      "Epoch 17/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1449 - acc: 0.9488 - val_loss: 0.1894 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.19185 to 0.18935, saving model to AlexNet_based_check_point/17-0.1894.hdf5\n",
      "Epoch 18/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1128 - acc: 0.9637 - val_loss: 0.1562 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.18935 to 0.15621, saving model to AlexNet_based_check_point/18-0.1562.hdf5\n",
      "Epoch 19/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1154 - acc: 0.9611 - val_loss: 0.1745 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.15621\n",
      "Epoch 20/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1058 - acc: 0.9637 - val_loss: 0.2051 - val_acc: 0.9384\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.15621\n",
      "Epoch 21/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0955 - acc: 0.9631 - val_loss: 0.1566 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.15621\n",
      "Epoch 22/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1053 - acc: 0.9631 - val_loss: 0.2092 - val_acc: 0.9384\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.15621\n",
      "Epoch 23/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0904 - acc: 0.9676 - val_loss: 0.1783 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.15621\n",
      "Epoch 24/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1227 - acc: 0.9598 - val_loss: 0.1851 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.15621\n",
      "Epoch 25/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0842 - acc: 0.9669 - val_loss: 0.2749 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.15621\n",
      "Epoch 26/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0870 - acc: 0.9682 - val_loss: 0.1652 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.15621\n",
      "Epoch 27/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0835 - acc: 0.9682 - val_loss: 0.2263 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.15621\n",
      "Epoch 28/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0860 - acc: 0.9689 - val_loss: 0.4204 - val_acc: 0.8361\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.15621\n",
      "Epoch 29/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0824 - acc: 0.9721 - val_loss: 0.2131 - val_acc: 0.9300\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.15621\n",
      "Epoch 30/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1002 - acc: 0.9644 - val_loss: 0.1791 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.15621\n",
      "Epoch 31/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0712 - acc: 0.9708 - val_loss: 0.1533 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.15621 to 0.15334, saving model to AlexNet_based_check_point/31-0.1533.hdf5\n",
      "Epoch 32/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0707 - acc: 0.9721 - val_loss: 0.1460 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.15334 to 0.14599, saving model to AlexNet_based_check_point/32-0.1460.hdf5\n",
      "Epoch 33/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0784 - acc: 0.9715 - val_loss: 0.7461 - val_acc: 0.8908\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.14599\n",
      "Epoch 34/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1187 - acc: 0.9611 - val_loss: 0.9089 - val_acc: 0.6919\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.14599\n",
      "Epoch 35/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1514 - acc: 0.9449 - val_loss: 0.1664 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.14599\n",
      "Epoch 36/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0724 - acc: 0.9734 - val_loss: 0.1635 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.14599\n",
      "Epoch 37/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0735 - acc: 0.9734 - val_loss: 0.2051 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.14599\n",
      "Epoch 38/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0678 - acc: 0.9741 - val_loss: 0.1466 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.14599\n",
      "Epoch 39/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0604 - acc: 0.9760 - val_loss: 0.1527 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.14599\n",
      "Epoch 40/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0565 - acc: 0.9767 - val_loss: 0.1724 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.14599\n",
      "Epoch 41/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0677 - acc: 0.9741 - val_loss: 0.1921 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.14599\n",
      "Epoch 42/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0636 - acc: 0.9734 - val_loss: 0.1505 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.14599\n",
      "Epoch 43/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0613 - acc: 0.9806 - val_loss: 0.1788 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.14599\n",
      "Epoch 44/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0606 - acc: 0.9773 - val_loss: 0.1959 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.14599\n",
      "Epoch 45/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0650 - acc: 0.9734 - val_loss: 0.1599 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.14599\n",
      "Epoch 46/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0719 - acc: 0.9773 - val_loss: 0.1744 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.14599\n",
      "Epoch 47/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0530 - acc: 0.9793 - val_loss: 0.2044 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.14599\n",
      "Epoch 48/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0738 - acc: 0.9702 - val_loss: 1.4769 - val_acc: 0.5504\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.14599\n",
      "Epoch 49/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1125 - acc: 0.9669 - val_loss: 0.3521 - val_acc: 0.8894\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.14599\n",
      "Epoch 50/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0604 - acc: 0.9786 - val_loss: 0.3491 - val_acc: 0.8880\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.14599\n",
      "Epoch 51/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0559 - acc: 0.9825 - val_loss: 0.1995 - val_acc: 0.9356\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.14599\n",
      "Epoch 52/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0590 - acc: 0.9799 - val_loss: 0.1686 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.14599\n",
      "Epoch 53/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0490 - acc: 0.9825 - val_loss: 0.1648 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.14599\n",
      "Epoch 54/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0567 - acc: 0.9812 - val_loss: 0.9272 - val_acc: 0.7283\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.14599\n",
      "Epoch 55/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0828 - acc: 0.9721 - val_loss: 0.2412 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.14599\n",
      "Epoch 56/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0831 - acc: 0.9689 - val_loss: 0.1621 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.14599\n",
      "Epoch 57/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0607 - acc: 0.9754 - val_loss: 1.2558 - val_acc: 0.5546\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.14599\n",
      "Epoch 58/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0607 - acc: 0.9793 - val_loss: 0.1596 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.14599\n",
      "Epoch 59/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0575 - acc: 0.9799 - val_loss: 0.1659 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.14599\n",
      "Epoch 60/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0694 - acc: 0.9741 - val_loss: 0.1760 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.14599\n",
      "Epoch 61/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0560 - acc: 0.9773 - val_loss: 0.1533 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.14599\n",
      "Epoch 62/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0609 - acc: 0.9793 - val_loss: 0.4982 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.14599\n",
      "Epoch 63/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0579 - acc: 0.9799 - val_loss: 0.1740 - val_acc: 0.9412\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.14599\n",
      "Epoch 64/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0618 - acc: 0.9806 - val_loss: 2.5436 - val_acc: 0.4048\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.14599\n",
      "Epoch 65/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.2490 - acc: 0.9469 - val_loss: 0.3220 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.14599\n",
      "Epoch 66/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0635 - acc: 0.9760 - val_loss: 0.2748 - val_acc: 0.9328\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.14599\n",
      "Epoch 67/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0587 - acc: 0.9819 - val_loss: 0.2553 - val_acc: 0.9370\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.14599\n",
      "Epoch 68/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0536 - acc: 0.9806 - val_loss: 0.1974 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.14599\n",
      "Epoch 69/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0557 - acc: 0.9786 - val_loss: 0.2062 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.14599\n",
      "Epoch 70/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0785 - acc: 0.9734 - val_loss: 0.4297 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.14599\n",
      "Epoch 71/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0671 - acc: 0.9741 - val_loss: 0.1466 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.14599\n",
      "Epoch 72/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0512 - acc: 0.9812 - val_loss: 0.1456 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.14599 to 0.14564, saving model to AlexNet_based_check_point/72-0.1456.hdf5\n",
      "Epoch 73/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0506 - acc: 0.9806 - val_loss: 0.1538 - val_acc: 0.9566\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.14564\n",
      "Epoch 74/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0407 - acc: 0.9831 - val_loss: 0.4369 - val_acc: 0.8165\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.14564\n",
      "Epoch 75/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0697 - acc: 0.9747 - val_loss: 1.7750 - val_acc: 0.4146\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.14564\n",
      "Epoch 76/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0869 - acc: 0.9631 - val_loss: 0.1638 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.14564\n",
      "Epoch 77/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0509 - acc: 0.9793 - val_loss: 0.1998 - val_acc: 0.9384\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.14564\n",
      "Epoch 78/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0451 - acc: 0.9844 - val_loss: 0.3437 - val_acc: 0.9356\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.14564\n",
      "Epoch 79/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0480 - acc: 0.9844 - val_loss: 0.1524 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.14564\n",
      "Epoch 80/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0507 - acc: 0.9825 - val_loss: 0.1699 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.14564\n",
      "Epoch 81/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0468 - acc: 0.9851 - val_loss: 0.1698 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.14564\n",
      "Epoch 82/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0499 - acc: 0.9831 - val_loss: 2.8693 - val_acc: 0.3908\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.14564\n",
      "Epoch 83/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.1980 - acc: 0.9553 - val_loss: 0.2605 - val_acc: 0.9328\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.14564\n",
      "Epoch 84/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0537 - acc: 0.9799 - val_loss: 0.2046 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.14564\n",
      "Epoch 85/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0590 - acc: 0.9780 - val_loss: 0.1609 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.14564\n",
      "Epoch 86/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0456 - acc: 0.9831 - val_loss: 0.1685 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.14564\n",
      "Epoch 87/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0484 - acc: 0.9825 - val_loss: 0.1496 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.14564\n",
      "Epoch 88/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0528 - acc: 0.9825 - val_loss: 0.1542 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.14564\n",
      "Epoch 89/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0439 - acc: 0.9819 - val_loss: 0.2051 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.14564\n",
      "Epoch 90/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0455 - acc: 0.9844 - val_loss: 0.1769 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.14564\n",
      "Epoch 91/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0376 - acc: 0.9870 - val_loss: 0.1624 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.14564\n",
      "Epoch 92/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0435 - acc: 0.9851 - val_loss: 0.1668 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.14564\n",
      "Epoch 93/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0910 - acc: 0.9728 - val_loss: 0.1496 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.14564\n",
      "Epoch 94/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0926 - acc: 0.9702 - val_loss: 0.1562 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.14564\n",
      "Epoch 95/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0442 - acc: 0.9851 - val_loss: 0.9199 - val_acc: 0.6443\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.14564\n",
      "Epoch 96/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0620 - acc: 0.9760 - val_loss: 0.1450 - val_acc: 0.9566\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.14564 to 0.14503, saving model to AlexNet_based_check_point/96-0.1450.hdf5\n",
      "Epoch 97/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0360 - acc: 0.9890 - val_loss: 0.1614 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.14503\n",
      "Epoch 98/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0448 - acc: 0.9806 - val_loss: 0.1624 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.14503\n",
      "Epoch 99/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0338 - acc: 0.9870 - val_loss: 0.1553 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.14503\n",
      "Epoch 100/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0325 - acc: 0.9857 - val_loss: 0.1599 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.14503\n",
      "Epoch 101/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0400 - acc: 0.9870 - val_loss: 0.1681 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.14503\n",
      "Epoch 102/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0347 - acc: 0.9857 - val_loss: 0.4283 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.14503\n",
      "Epoch 103/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0532 - acc: 0.9806 - val_loss: 0.1618 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.14503\n",
      "Epoch 104/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0491 - acc: 0.9799 - val_loss: 0.1653 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.14503\n",
      "Epoch 105/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0281 - acc: 0.9896 - val_loss: 0.2650 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.14503\n",
      "Epoch 106/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0653 - acc: 0.9786 - val_loss: 0.1591 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.14503\n",
      "Epoch 107/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0317 - acc: 0.9916 - val_loss: 0.1714 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.14503\n",
      "Epoch 108/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0284 - acc: 0.9883 - val_loss: 0.1651 - val_acc: 0.9566\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.14503\n",
      "Epoch 109/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0394 - acc: 0.9864 - val_loss: 0.1725 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.14503\n",
      "Epoch 110/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0412 - acc: 0.9851 - val_loss: 0.1673 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.14503\n",
      "Epoch 111/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0292 - acc: 0.9877 - val_loss: 0.3587 - val_acc: 0.8796\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.14503\n",
      "Epoch 112/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0435 - acc: 0.9819 - val_loss: 0.1840 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.14503\n",
      "Epoch 113/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0312 - acc: 0.9896 - val_loss: 0.1877 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.14503\n",
      "Epoch 114/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0328 - acc: 0.9877 - val_loss: 0.1685 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.14503\n",
      "Epoch 115/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0428 - acc: 0.9870 - val_loss: 0.1752 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.14503\n",
      "Epoch 116/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0297 - acc: 0.9922 - val_loss: 0.1983 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.14503\n",
      "Epoch 117/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0266 - acc: 0.9890 - val_loss: 0.1814 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.14503\n",
      "Epoch 118/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0178 - acc: 0.9961 - val_loss: 0.2127 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.14503\n",
      "Epoch 119/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0190 - acc: 0.9942 - val_loss: 0.2107 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.14503\n",
      "Epoch 120/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0222 - acc: 0.9896 - val_loss: 0.2190 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.14503\n",
      "Epoch 121/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0248 - acc: 0.9896 - val_loss: 0.2129 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.14503\n",
      "Epoch 122/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0145 - acc: 0.9968 - val_loss: 0.1897 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.14503\n",
      "Epoch 123/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0283 - acc: 0.9890 - val_loss: 0.2452 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.14503\n",
      "Epoch 124/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0209 - acc: 0.9909 - val_loss: 0.2058 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.14503\n",
      "Epoch 125/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0330 - acc: 0.9877 - val_loss: 0.4158 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.14503\n",
      "Epoch 126/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0625 - acc: 0.9806 - val_loss: 0.1889 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.14503\n",
      "Epoch 127/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0297 - acc: 0.9916 - val_loss: 0.1897 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.14503\n",
      "Epoch 128/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0205 - acc: 0.9916 - val_loss: 0.2230 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.14503\n",
      "Epoch 129/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0239 - acc: 0.9903 - val_loss: 0.2596 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.14503\n",
      "Epoch 130/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0395 - acc: 0.9838 - val_loss: 0.2150 - val_acc: 0.9566\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.14503\n",
      "Epoch 131/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0422 - acc: 0.9877 - val_loss: 0.5847 - val_acc: 0.8249\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.14503\n",
      "Epoch 132/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0356 - acc: 0.9877 - val_loss: 0.2110 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.14503\n",
      "Epoch 133/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0173 - acc: 0.9942 - val_loss: 0.2656 - val_acc: 0.9230\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.14503\n",
      "Epoch 134/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0300 - acc: 0.9916 - val_loss: 0.2489 - val_acc: 0.9328\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.14503\n",
      "Epoch 135/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0341 - acc: 0.9870 - val_loss: 1.8700 - val_acc: 0.5658\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.14503\n",
      "Epoch 136/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0517 - acc: 0.9825 - val_loss: 0.4361 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.14503\n",
      "Epoch 137/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0383 - acc: 0.9831 - val_loss: 0.1961 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.14503\n",
      "Epoch 138/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0176 - acc: 0.9929 - val_loss: 0.1941 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.14503\n",
      "Epoch 139/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0173 - acc: 0.9935 - val_loss: 0.2076 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.14503\n",
      "Epoch 140/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0439 - acc: 0.9857 - val_loss: 0.8372 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.14503\n",
      "Epoch 141/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0323 - acc: 0.9896 - val_loss: 0.6265 - val_acc: 0.8235\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.14503\n",
      "Epoch 142/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0249 - acc: 0.9896 - val_loss: 0.1955 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.14503\n",
      "Epoch 143/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0209 - acc: 0.9916 - val_loss: 0.2787 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.14503\n",
      "Epoch 144/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0304 - acc: 0.9903 - val_loss: 0.1902 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.14503\n",
      "Epoch 145/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0161 - acc: 0.9955 - val_loss: 0.2309 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.14503\n",
      "Epoch 146/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0171 - acc: 0.9955 - val_loss: 0.2051 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.14503\n",
      "Epoch 147/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0136 - acc: 0.9955 - val_loss: 0.2268 - val_acc: 0.9384\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.14503\n",
      "Epoch 148/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0169 - acc: 0.9955 - val_loss: 0.2776 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.14503\n",
      "Epoch 149/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0106 - acc: 0.9948 - val_loss: 0.2313 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.14503\n",
      "Epoch 150/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0171 - acc: 0.9922 - val_loss: 0.3475 - val_acc: 0.9370\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.14503\n",
      "Epoch 151/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0337 - acc: 0.9909 - val_loss: 0.2151 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.14503\n",
      "Epoch 152/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0272 - acc: 0.9916 - val_loss: 0.2190 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.14503\n",
      "Epoch 153/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0307 - acc: 0.9909 - val_loss: 0.2399 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.14503\n",
      "Epoch 154/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0108 - acc: 0.9981 - val_loss: 0.2333 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.14503\n",
      "Epoch 155/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0141 - acc: 0.9942 - val_loss: 0.2965 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.14503\n",
      "Epoch 156/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0215 - acc: 0.9935 - val_loss: 0.2188 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.14503\n",
      "Epoch 157/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0118 - acc: 0.9961 - val_loss: 0.3032 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.14503\n",
      "Epoch 158/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0096 - acc: 0.9968 - val_loss: 0.2517 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.14503\n",
      "Epoch 159/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0089 - acc: 0.9981 - val_loss: 0.3097 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.14503\n",
      "Epoch 160/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0131 - acc: 0.9961 - val_loss: 1.2437 - val_acc: 0.7129\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.14503\n",
      "Epoch 161/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0703 - acc: 0.9851 - val_loss: 0.3525 - val_acc: 0.8950\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.14503\n",
      "Epoch 162/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0140 - acc: 0.9948 - val_loss: 0.2338 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.14503\n",
      "Epoch 163/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0136 - acc: 0.9955 - val_loss: 0.2404 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.14503\n",
      "Epoch 164/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0105 - acc: 0.9968 - val_loss: 0.2619 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.14503\n",
      "Epoch 165/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0130 - acc: 0.9961 - val_loss: 0.2325 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.14503\n",
      "Epoch 166/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0273 - acc: 0.9870 - val_loss: 0.3055 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.14503\n",
      "Epoch 167/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0128 - acc: 0.9948 - val_loss: 0.2155 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.14503\n",
      "Epoch 168/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0919 - acc: 0.9825 - val_loss: 0.2859 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.14503\n",
      "Epoch 169/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0126 - acc: 0.9961 - val_loss: 0.2629 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.14503\n",
      "Epoch 170/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0097 - acc: 0.9968 - val_loss: 0.2696 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.14503\n",
      "Epoch 171/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0140 - acc: 0.9942 - val_loss: 0.2141 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.14503\n",
      "Epoch 172/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0266 - acc: 0.9916 - val_loss: 0.2505 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.14503\n",
      "Epoch 173/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0081 - acc: 0.9987 - val_loss: 0.2473 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.14503\n",
      "Epoch 174/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0324 - acc: 0.9909 - val_loss: 0.2220 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.14503\n",
      "Epoch 175/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0105 - acc: 0.9961 - val_loss: 0.2093 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.14503\n",
      "Epoch 176/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0080 - acc: 0.9974 - val_loss: 0.2429 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.14503\n",
      "Epoch 177/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0187 - acc: 0.9922 - val_loss: 0.2818 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.14503\n",
      "Epoch 178/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0092 - acc: 0.9968 - val_loss: 7.1688 - val_acc: 0.3838\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.14503\n",
      "Epoch 179/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0790 - acc: 0.9806 - val_loss: 0.3149 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.14503\n",
      "Epoch 180/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0178 - acc: 0.9948 - val_loss: 0.2142 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.14503\n",
      "Epoch 181/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0144 - acc: 0.9942 - val_loss: 5.0315 - val_acc: 0.3880\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.14503\n",
      "Epoch 182/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.3672 - acc: 0.9579 - val_loss: 0.2618 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.14503\n",
      "Epoch 183/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0306 - acc: 0.9909 - val_loss: 0.2957 - val_acc: 0.9412\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.14503\n",
      "Epoch 184/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0199 - acc: 0.9942 - val_loss: 0.2253 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.14503\n",
      "Epoch 185/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0125 - acc: 0.9968 - val_loss: 0.2483 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.14503\n",
      "Epoch 186/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0103 - acc: 0.9968 - val_loss: 0.2900 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.14503\n",
      "Epoch 187/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0109 - acc: 0.9968 - val_loss: 0.2201 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.14503\n",
      "Epoch 188/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0261 - acc: 0.9896 - val_loss: 0.2230 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.14503\n",
      "Epoch 189/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0153 - acc: 0.9961 - val_loss: 0.4493 - val_acc: 0.9300\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.14503\n",
      "Epoch 190/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0316 - acc: 0.9929 - val_loss: 0.2301 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.14503\n",
      "Epoch 191/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0370 - acc: 0.9903 - val_loss: 0.2058 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.14503\n",
      "Epoch 192/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0113 - acc: 0.9968 - val_loss: 0.2285 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.14503\n",
      "Epoch 193/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0238 - acc: 0.9955 - val_loss: 0.2129 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.14503\n",
      "Epoch 194/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1115 - val_acc: 0.6989\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.14503\n",
      "Epoch 195/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0211 - acc: 0.9935 - val_loss: 0.1915 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.14503\n",
      "Epoch 196/10000\n",
      "1543/1543 [==============================] - 4s 3ms/step - loss: 0.0124 - acc: 0.9968 - val_loss: 0.2057 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.14503\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, batch_size=64, epochs=10000, validation_data=[x_val, y_val], \n",
    "                 shuffle=True, callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make test data.......\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27dff945c12d466e941b20d4e1ab0ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1054), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Make test data.......')\n",
    "x_test_wav_filenames = [test_dir+filename for filename in os.listdir(test_dir)\n",
    "                            if filename.endswith('.wav')]\n",
    "x_test, y_test = make_xy_data(x_test_wav_filenames, y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model(model_path+'96-0.1450.hdf5',custom_objects={'<lambda>': ParametricSoftplus(0.2, 0.5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DefsBoWAFKdi",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054/1054 [==============================] - 1s 1ms/step\n",
      "Loss: 0.09208296200497326 Accuracy: 0.9705882352941176\n"
     ]
    }
   ],
   "source": [
    "[loss, accuracy] = model.evaluate(x_test, y_test)\n",
    "print('Loss:', loss, 'Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9756480754124115\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "test_f1_score = f1_score(y_test, pred > 0.5)\n",
    "print('F1 Score:', test_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783 ms ± 13 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 5 -r 5 model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "brains_on_beats_model_test",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
